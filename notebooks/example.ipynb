{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuYLZC_Ks3F1"
      },
      "source": [
        "# JIND-Multi Notebook Tutorial\n",
        "\n",
        "This notebook provides a step-by-step guide on how to run the JIND-Multi method using the Pancreas scRNA-seq dataset as an example. Specifically, batch 0 will be used as the source, batch 2 as the target, and batch 1 as an additional intermediate dataset. Since we have labels for the target batch, we will use confusion matrices to evaluate the results.\n",
        "\n",
        "## 1. Initial Setup\n",
        "\n",
        "First, let's ensure we have the necessary dependencies and import the `jind_multi` package."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wk0QmC-dtB1_",
        "outputId": "f6083ce9-5267-4b7c-a636-d26fbb9785b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pandas==1.3.5\n",
            "  Downloading pandas-1.3.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting scanpy==1.8.0\n",
            "  Downloading scanpy-1.8.0-py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.10/dist-packages (from pandas==1.3.5) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.10/dist-packages (from pandas==1.3.5) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas==1.3.5) (1.26.4)\n",
            "Collecting anndata>=0.7.4 (from scanpy==1.8.0)\n",
            "  Downloading anndata-0.10.9-py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: matplotlib>=3.1.2 in /usr/local/lib/python3.10/dist-packages (from scanpy==1.8.0) (3.7.1)\n",
            "Requirement already satisfied: scipy>=1.4 in /usr/local/lib/python3.10/dist-packages (from scanpy==1.8.0) (1.13.1)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (from scanpy==1.8.0) (0.13.1)\n",
            "Requirement already satisfied: h5py>=2.10.0 in /usr/local/lib/python3.10/dist-packages (from scanpy==1.8.0) (3.11.0)\n",
            "Requirement already satisfied: tables in /usr/local/lib/python3.10/dist-packages (from scanpy==1.8.0) (3.8.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from scanpy==1.8.0) (4.66.5)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.10/dist-packages (from scanpy==1.8.0) (1.3.2)\n",
            "Requirement already satisfied: statsmodels>=0.10.0rc2 in /usr/local/lib/python3.10/dist-packages (from scanpy==1.8.0) (0.14.2)\n",
            "Requirement already satisfied: patsy in /usr/local/lib/python3.10/dist-packages (from scanpy==1.8.0) (0.5.6)\n",
            "Requirement already satisfied: networkx>=2.3 in /usr/local/lib/python3.10/dist-packages (from scanpy==1.8.0) (3.3)\n",
            "Requirement already satisfied: natsort in /usr/local/lib/python3.10/dist-packages (from scanpy==1.8.0) (8.4.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from scanpy==1.8.0) (1.4.2)\n",
            "Requirement already satisfied: numba>=0.41.0 in /usr/local/lib/python3.10/dist-packages (from scanpy==1.8.0) (0.60.0)\n",
            "Collecting umap-learn>=0.3.10 (from scanpy==1.8.0)\n",
            "  Downloading umap_learn-0.5.6-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting legacy-api-wrap (from scanpy==1.8.0)\n",
            "  Downloading legacy_api_wrap-1.4-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from scanpy==1.8.0) (24.1)\n",
            "Collecting sinfo (from scanpy==1.8.0)\n",
            "  Downloading sinfo-0.3.4.tar.gz (24 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting array-api-compat!=1.5,>1.4 (from anndata>=0.7.4->scanpy==1.8.0)\n",
            "  Downloading array_api_compat-1.8-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anndata>=0.7.4->scanpy==1.8.0) (1.2.2)\n",
            "INFO: pip is looking at multiple versions of anndata to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting anndata>=0.7.4 (from scanpy==1.8.0)\n",
            "  Downloading anndata-0.10.8-py3-none-any.whl.metadata (6.6 kB)\n",
            "  Downloading anndata-0.10.7-py3-none-any.whl.metadata (6.6 kB)\n",
            "  Downloading anndata-0.10.6-py3-none-any.whl.metadata (6.6 kB)\n",
            "  Downloading anndata-0.10.5.post1-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.2->scanpy==1.8.0) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.2->scanpy==1.8.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.2->scanpy==1.8.0) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.2->scanpy==1.8.0) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.2->scanpy==1.8.0) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.2->scanpy==1.8.0) (3.1.4)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.41.0->scanpy==1.8.0) (0.43.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7.3->pandas==1.3.5) (1.16.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22->scanpy==1.8.0) (3.5.0)\n",
            "INFO: pip is looking at multiple versions of statsmodels to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting statsmodels>=0.10.0rc2 (from scanpy==1.8.0)\n",
            "  Downloading statsmodels-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.5 kB)\n",
            "Collecting pynndescent>=0.5 (from umap-learn>=0.3.10->scanpy==1.8.0)\n",
            "  Downloading pynndescent-0.5.13-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting stdlib_list (from sinfo->scanpy==1.8.0)\n",
            "  Downloading stdlib_list-0.10.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: cython>=0.29.21 in /usr/local/lib/python3.10/dist-packages (from tables->scanpy==1.8.0) (3.0.11)\n",
            "Requirement already satisfied: numexpr>=2.6.2 in /usr/local/lib/python3.10/dist-packages (from tables->scanpy==1.8.0) (2.10.1)\n",
            "Requirement already satisfied: blosc2~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from tables->scanpy==1.8.0) (2.0.0)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from tables->scanpy==1.8.0) (9.0.0)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.10/dist-packages (from blosc2~=2.0.0->tables->scanpy==1.8.0) (1.0.8)\n",
            "Downloading pandas-1.3.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m82.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scanpy-1.8.0-py3-none-any.whl (11.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading anndata-0.10.5.post1-py3-none-any.whl (121 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.0/121.0 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading statsmodels-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m85.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading umap_learn-0.5.6-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.7/85.7 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading legacy_api_wrap-1.4-py3-none-any.whl (15 kB)\n",
            "Downloading pynndescent-0.5.13-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.9/56.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading array_api_compat-1.8-py3-none-any.whl (38 kB)\n",
            "Downloading stdlib_list-0.10.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: sinfo\n",
            "  Building wheel for sinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sinfo: filename=sinfo-0.3.4-py3-none-any.whl size=7879 sha256=94f98d7c754cc59475352b65262cbc8d817af5e5ce8677c41207c83f9962c582\n",
            "  Stored in directory: /root/.cache/pip/wheels/ae/fe/9d/eb4b47396d5c94b8ad82a5aa9f905c56c981deb4e532329f72\n",
            "Successfully built sinfo\n",
            "Installing collected packages: array-api-compat, stdlib_list, legacy-api-wrap, sinfo, pandas, statsmodels, pynndescent, anndata, umap-learn, scanpy\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.1.4\n",
            "    Uninstalling pandas-2.1.4:\n",
            "      Successfully uninstalled pandas-2.1.4\n",
            "  Attempting uninstall: statsmodels\n",
            "    Found existing installation: statsmodels 0.14.2\n",
            "    Uninstalling statsmodels-0.14.2:\n",
            "      Successfully uninstalled statsmodels-0.14.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "arviz 0.18.0 requires pandas>=1.5.0, but you have pandas 1.3.5 which is incompatible.\n",
            "bigframes 1.15.0 requires pandas>=1.5.0, but you have pandas 1.3.5 which is incompatible.\n",
            "cudf-cu12 24.4.1 requires pandas<2.2.2dev0,>=2.0, but you have pandas 1.3.5 which is incompatible.\n",
            "geopandas 0.14.4 requires pandas>=1.4.0, but you have pandas 1.3.5 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==2.1.4, but you have pandas 1.3.5 which is incompatible.\n",
            "plotnine 0.12.4 requires pandas>=1.5.0, but you have pandas 1.3.5 which is incompatible.\n",
            "xarray 2024.6.0 requires pandas>=2.0, but you have pandas 1.3.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed anndata-0.10.5.post1 array-api-compat-1.8 legacy-api-wrap-1.4 pandas-1.3.5 pynndescent-0.5.13 scanpy-1.8.0 sinfo-0.3.4 statsmodels-0.14.1 stdlib_list-0.10.0 umap-learn-0.5.6\n"
          ]
        }
      ],
      "source": [
        "#To use in google colab install this 2 packgages\n",
        "#!pip install pandas==1.3.5 \\\n",
        "#  scanpy==1.8.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "id": "f4PrD0AHs3F3",
        "outputId": "ca0a649a-3bf7-4305-ce4b-ed89f4bac1c7"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'scanpy'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-9c7d8fc9cedc>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Import the jind_multi package\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mjind_multi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/jind_multi/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# jind_multi/__init__.py\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrun_main\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconfig_loader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfind_saved_models\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_trained_models\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_val_stats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mjind_wrapper\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mJindWrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/jind_multi/core.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfind_saved_models\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_trained_models\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_val_stats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mjind_wrapper\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mJindWrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdata_loader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_and_process_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/jind_multi/utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mscanpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mitertools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mproduct\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'scanpy'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "import ast\n",
        "\n",
        "# Get the path to the project root directory\n",
        "project_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
        "\n",
        "# Add the path to sys.path\n",
        "if project_dir not in sys.path:\n",
        "    sys.path.append(project_dir)\n",
        "\n",
        "# Import the jind_multi package\n",
        "import jind_multi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfX6atu_s3F4"
      },
      "source": [
        "# 2. Configuring Parameters\n",
        "\n",
        "In this section, we define the inputs required for running JIND-Multi by specifying various parameters. These inputs include:\n",
        "\n",
        "- **Path to the `.h5ad` file**: This is the location of the data file containing the single-cell RNA sequencing data.\n",
        "- **Batch and cell type column names**: We specify the column names for batch information and cell types within the AnnData object.\n",
        "- **Source and target batch names**: We indicate which batch will be used as the source for annotation transfer and which batch will be the target for annotation.\n",
        "- **Output path**: This is where the results of the analysis will be saved.\n",
        "- **Training configurations**: These include the number of features to consider in the model, the minimum number of cells required per cell type for training in each batch, and whether to use a GPU for computation.\n",
        "- **Intermediate datasets**: The `TRAIN_DATASETS_NAMES` parameter specifies which batches are used as intermediate datasets for training. These datasets, excluding the source and target batches, help in improving the model’s performance by providing additional training data.\n",
        "\n",
        "We define these parameters in the `Args` class, which will be used to configure and run the JIND-Multi method.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sIw4mbg5s3F4"
      },
      "outputs": [],
      "source": [
        "class Args:\n",
        "    PATH = \"../resources/data/pancreas/pancreas.h5ad\"  # path to your data\n",
        "    BATCH_COL = \"batch\" # Column name for batch information in the AnnData object\n",
        "    LABELS_COL = \"celltype\" # Column name for cell types in the AnnData object\n",
        "    SOURCE_DATASET_NAME = \"0\" # Name of the source batch\n",
        "    TARGET_DATASET_NAME = \"2\" # Name of the target batch\n",
        "    OUTPUT_PATH = \"../results/pancreas\" # Directory to save results\n",
        "    PRETRAINED_MODEL_PATH = \"\" # Path to pre-trained models, if available (here we are not introducing any)\n",
        "    TRAIN_DATASETS_NAMES = \"['1']\" # List of intermediate datasets for training\n",
        "    NUM_FEATURES = 5000 # Number of features (genes) to consider for modeling\n",
        "    MIN_CELL_TYPE_POPULATION = 100 # Minimum number of cells required per cell type for training\n",
        "    USE_GPU = True # Whether to use GPU for computation\n",
        "\n",
        "args = Args()\n",
        "print(args.PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usN5Kflls3F5"
      },
      "source": [
        "# 3. Setting Up the Training Configuration\n",
        "We adjust the training configuration according to the specified parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E1aCTHxFs3F5"
      },
      "outputs": [],
      "source": [
        "# Set up training configuration (you can modify more things here)\n",
        "config = jind_multi.get_config()\n",
        "config['data']['num_features'] = args.NUM_FEATURES\n",
        "config['data']['min_cell_type_population'] = args.MIN_CELL_TYPE_POPULATION\n",
        "config['train_classifier']['cuda'] = args.USE_GPU\n",
        "config['GAN']['cuda'] = args.USE_GPU\n",
        "config['ftune']['cuda'] = args.USE_GPU\n",
        "print(f'USE_GPU: {args.USE_GPU}')\n",
        "print(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jq1J_iXDs3F5"
      },
      "source": [
        "# 4. Loading and Processing Data\n",
        "We load and process the data using the `load_and_process_data` function from the `jind_multi` package. Then, we divide the data into training and test sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2sYFjmn4s3F6"
      },
      "outputs": [],
      "source": [
        "# Load and process the data\n",
        "data = jind_multi.load_and_process_data(args, config)\n",
        "\n",
        "# Split into training and test datasets\n",
        "train_data = data[data['batch'] != args.TARGET_DATASET_NAME]\n",
        "test_data = data[data['batch'] == args.TARGET_DATASET_NAME]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQKQVg05s3F6"
      },
      "source": [
        "# 5. Creating the JIND-Multi Object\n",
        "We create an instance of the JindWrapper class, which encapsulates the functionality of JIND-Multi, including training and evaluating the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k9BnYmMss3F6"
      },
      "outputs": [],
      "source": [
        "train_datasets_names = ast.literal_eval(args.TRAIN_DATASETS_NAMES)\n",
        "\n",
        "jind = jind_multi.JindWrapper(\n",
        "    train_data=train_data,\n",
        "    train_dataset_names=train_datasets_names,\n",
        "    source_dataset_name=args.SOURCE_DATASET_NAME,\n",
        "    output_path=args.OUTPUT_PATH,\n",
        "    config=config\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pM62YUOBs3F7"
      },
      "source": [
        "# 6. Training the Model\n",
        "Now we train the model and we infer the labels on the test dataset (target batch)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mrSoG6Ics3F7"
      },
      "outputs": [],
      "source": [
        "# Train the JIND-Multi model\n",
        "jind.train(target_data=test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DoAwbqxs3F7"
      },
      "source": [
        "## 7. Applying the Trained Model to a New Target Batch\n",
        "\n",
        "Once the model has been trained, you may want to apply it to a new target batch. This process involves loading the pre-trained model from the saved directory and using it to make predictions on new data.\n",
        "\n",
        "### Steps:\n",
        "\n",
        "1. **Specify the Path to Pre-Trained Models**: Ensure that the path to the directory containing the trained models is correctly set. This directory should include the model files (`.pt` format) and the associated configuration files.\n",
        "\n",
        "2. **Load the Pre-Trained Model**: Use the `JindWrapper` class to load the pre-trained model from the specified directory.\n",
        "\n",
        "3. **Apply the Model to the New Target Batch**: Run the model on the new target batch to get predictions.\n",
        "\n",
        "Here’s how you can do this in code for batch 3:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HcZ6DzCAs3F7"
      },
      "outputs": [],
      "source": [
        "# Define new parameters for applying the model to a new target batch\n",
        "class Args:\n",
        "    PATH = \"../resources/data/pancreas/pancreas.h5ad\"  # path to your data\n",
        "    BATCH_COL = \"batch\" # Column name for batch information in the AnnData object\n",
        "    LABELS_COL = \"celltype\" # Column name for cell types in the AnnData object\n",
        "    SOURCE_DATASET_NAME = \"0\" # Name of the source batch\n",
        "    TARGET_DATASET_NAME = \"3\" # Name of the target batch\n",
        "    OUTPUT_PATH = \"../results/pancreas_target3\" # Directory to save results\n",
        "    PRETRAINED_MODEL_PATH = \"../results/pancreas/trained_models\" # Path to pre-trained models, if available (here we are not introducing any)\n",
        "    TRAIN_DATASETS_NAMES = \"['1']\" # List of intermediate datasets for training\n",
        "    NUM_FEATURES = 5000 # Number of features (genes) to consider for modeling\n",
        "    MIN_CELL_TYPE_POPULATION = 5 # Minimum number of cells required per cell type for training\n",
        "    USE_GPU = True # Whether to use GPU for computation\n",
        "\n",
        "args = Args()\n",
        "print(args.PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TKndRNfis3F7"
      },
      "outputs": [],
      "source": [
        "# Set up training configuration\n",
        "config = jind_multi.get_config()\n",
        "config['data']['num_features'] = args.NUM_FEATURES\n",
        "config['data']['min_cell_type_population'] = args.MIN_CELL_TYPE_POPULATION\n",
        "config['train_classifier']['cuda'] = args.USE_GPU\n",
        "config['GAN']['cuda'] = args.USE_GPU\n",
        "config['ftune']['cuda'] = args.USE_GPU\n",
        "print(f'USE_GPU: {args.USE_GPU}')\n",
        "print(config)\n",
        "\n",
        "# Load and process the data\n",
        "data = jind_multi.load_and_process_data(args, config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bUHvxacGs3F7"
      },
      "outputs": [],
      "source": [
        "# Split into training and test datasets\n",
        "train_data = data[data['batch'] != args.TARGET_DATASET_NAME]\n",
        "test_data = data[data['batch'] == args.TARGET_DATASET_NAME]\n",
        "\n",
        "# Create the JIND-Multi object\n",
        "jind2 = jind_multi.JindWrapper(\n",
        "                                train_data=train_data,\n",
        "                                source_dataset_name=args.SOURCE_DATASET_NAME,\n",
        "                                output_path=args.OUTPUT_PATH,\n",
        "                                config=config,\n",
        "                            )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bl0sRJrWs3F7"
      },
      "source": [
        "### Loading and Applying the Pre-Trained Model\n",
        "\n",
        "After setting up your parameters and preparing the data, the next step is to load the pre-trained model and use it for predictions on the new target batch. This process involves several steps:\n",
        "\n",
        "This step identifies the files containing the pre-trained models stored in the specified directory and loads validation statistics used to evaluate the performance of the pre-trained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YqB9d_eTs3F7"
      },
      "outputs": [],
      "source": [
        "print('Loading pre-trained models from specified path...')\n",
        "file_paths = jind_multi.find_saved_models(args.PRETRAINED_MODEL_PATH, train_data)  # Check if pre-trained models are available\n",
        "print(file_paths)\n",
        "model = jind_multi.load_trained_models(file_paths, train_data, args.SOURCE_DATASET_NAME)\n",
        "print(model)\n",
        "print(\"Loading validation statistics...\")\n",
        "val_stats = jind_multi.load_val_stats(args.PRETRAINED_MODEL_PATH, 'val_stats_trained_model.json')\n",
        "print(val_stats)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3BSUrk3s3F8"
      },
      "source": [
        "**Applying the Model to the New Target Batch:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09WcnkBes3F8"
      },
      "outputs": [],
      "source": [
        "# Do JIND\n",
        "jind2.train(target_data=test_data, model=model, val_stats=val_stats)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbPQYVCIs3F8"
      },
      "source": [
        "# 8 Conclusion\n",
        "\n",
        "This notebook provided a comprehensive guide on configuring and running `JIND-Multi` for single-cell RNA sequencing analysis, using the Pancreas dataset with multiple labeled batches. It covered the key steps, including setting parameters, loading and processing data, and evaluating the model using confusion matrices. To tailor the analysis to your specific dataset and research objectives, adjust the parameters accordingly and review the results.\n",
        "\n",
        "For further guidance on interpreting the results, please consult the `JIND-Multi` package documentation and the output files located in the `OUTPUT_PATH`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GsOb2lNs3F8"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "jind",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
